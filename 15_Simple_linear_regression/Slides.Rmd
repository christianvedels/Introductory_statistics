---
output:
  xaringan::moon_reader:
    seal: false
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
editor_options:
  chunk_output_type: console
---
class: center, inverse, middle

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra, echo = FALSE}
xaringanExtra::use_progress_bar(color = "#808080", location = "top")
```

```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.tiny123 {
  font-size: 0.40em;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}

.orange {
  color: orange
}

.green {
  color: green
}
```



# Statistics
## Simple linear regression
### (Chapter 17)

### Seetha Menon,<br>Department of Economics<br>University of Southern Denmark

### Email: [smr@sam.sdu.dk](mailto:smr@sam.sdu.dk)

### Updated `r Sys.Date()`



---
class: inverse, middle, center
# Regression

---
# Motivation

--

- The relationship between two random variables $X$ and $Y$ is specified by their joint probability distribution $f(x, y)$

--

- We can also study the relationship between a given value of $X$ and the distribution of $Y$

--

- This relationship is given by the conditional distribution function $f_{Y|X}(y | x)$

--

- If $X$ and $Y$ are not independent, then knowing the value of $X$ gives us some information on the distribution of $Y$

---
# Regression function

--

- Regression analysis focuses on the conditional mean $E(Y | X = x)$

--

- This is a summary measure for the conditional distribution $f_{Y|X}(y | x)$, which makes it easier to interpret

--

- This function can be defined as follows:
  - if $Y$ is discrete: $\displaystyle E(Y|X = x) = \sum_{i=1}^N \left[ y_i \cdot f_{Y|X}(y_i|x) \right]$
  - if $Y$ is continuous: $\displaystyle E(Y|X = x) = \int_{-\infty}^\infty y \cdot f_{Y|X}(y|x) \, dy$

--

- In either case, note that $E(Y|X = x)$ is a function of $x$, and for this reason it is called **regression function** of $Y$ on $X$

--

- The two variables have different names:
  - $Y$ is the **dependent (explained)** variable
  - $X$ is the **independent (explanatory)** variable

---
# Error

--

- The regression function gives us the expected value of $Y$ for a given value of $X$

--

- The *actual* value of $Y$ for a given value of $X$ is rarely equal to that

--

- We can then define the error term or residual as the difference between the two:
$$U = Y - E(Y|X = x)$$

--

- By construction, this error term has zero conditional mean:
$$E(U | X = x) = E \! \left[ Y - E(Y|X = x) \right] = 0$$

--

- Note that the regression function is simply a way of describing a statistical relationship between $Y$ and $X$: this does not imply *causality* (for this, see the next course...)

---
class: inverse, middle, center
# Simple linear regression

---
# Linear regression

--

- If we want to say more about the relationship between $X$ and $Y$, we need to make some assumption about the nature of this relationship

--

- In practice, we often assume a linear relationship

--

- This leads to the **simple linear regression** of $Y$ on $X$:
$$E(Y | X = x) = \beta_0 + \beta_1 \cdot x$$

--

- The two betas also have specific names:
  - $\beta_0$ is called **intercept**
  - $\beta_1$ is called **slope coefficient**

---
# Coefficients

--

- It can be shown that the two coefficients can be calculated using only the variance of $X$ and the covariance between $X$ and $Y$:
$$\begin{align*}
  \beta_1 & = \frac{Cov(X, Y)}{Var(X)} \\
  \beta_0 & = E(Y) - \beta_1 E(X)
\end{align*}$$

--

- These formulas imply that:
  - the sign of $\beta_1$ gives the sign of the correlation between $X$ and $Y$
  - the effect of a unit change in $X$ on $Y$ is given by the magnitude of $\beta_1$

---
class: inverse, middle, center
# Estimation of the simple linear regression

---
# Analogy principle

--

- Suppose that we have a simple random sample:
$$\big( (X_1, Y_1), (X_2, Y_2), \ldots, (X_n, Y_n) \big)$$

--

- We can use the analogy principle to calculate the two coefficients

--

- First, we need to calculate the corresponding sample measures to the three unknown quantities:
$$\begin{align*}
  Cov(X, Y): & \; \widehat{Cov}(X, Y) = \frac{1}{n-1} \sum_{i=1}^n \left[(X_i - \bar{X}) \cdot (Y_i - \bar{Y}) \right] \\
  Var(X): & \; S_X^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2 \\
  E(Y): & \; \bar{Y} = \frac{1}{n} \sum_{i=1}^n Y_i
\end{align*}$$

---
# Estimators

--

- By replacing in the two formulas, we obtain estimators for the two coefficients:
$$\begin{align*}
  \hat{\beta}_1 & = \frac{\widehat{Cov}(X, Y)}{S_X^2} = \frac{\sum_{i=1}^n \left[(X_i - \bar{X}) \cdot (Y_i - \bar{Y}) \right]}{\sum_{i=1}^n (X_i - \bar{X})^2} \\
  \hat{\beta}_0 & = \bar{Y} - \hat{\beta}_1 \cdot \bar{X}
\end{align*}$$

--

- These estimators are called the **ordinary least squares (OLS)** estimators

--

- This is because they are the solution to minimizing the following sum of squares:
$$\sum_{i=1}^n \left[ Y_i - (b_0 + b_1 \cdot X_i) \right]^2$$

--

- This minimization problem is equivalent to the "best" fit of a straight line through the points represented by the elements of our sample

---
class: inverse, middle, center
# Hypothesis testing and confidence intervals

---
# Hypothesis tests

--

- Once we have estimated our coefficients, we can move on to testing for their value

--

- First, we may want to test whether $X$ and $Y$ are indeed related

--

- This is simply a test of the hypotheses:
$$\begin{align*}
  H_0 &: \beta_1 = 0 \\
  H_1 &: \beta_1 \not= 0
\end{align*}$$

--

- In general, we may want to test $H_0: \beta_1 = \beta_1^0$, where $\beta_1^0$ is a value suggested by theory

---
# Hypothesis tests

--

- It can be shown that $\hat{\beta}_1$ is an unbiased estimator of $\beta_1$, meaning that $E(\hat{\beta}_1) = \beta_1$

--

- The test above then becomes a test of the mean value of $\hat{\beta}_1$

--

- Therefore, we can construct a test statistic similar to before:
$$Z = \frac{\hat{\beta}_1 - \beta_1^0}{\sqrt{Var(\hat{\beta}_1 - \beta_1^0)}} = \frac{\hat{\beta}_1 - \beta_1^0}{\sqrt{S_{\hat{\beta}_1}^2}}$$

--

- This test statistic follows an approximate standard normal distribution

--

- Hence:
  - do not reject $H_0$ if $z_{\alpha/2} \leq Z \leq z_{1 - \alpha/2}$
  - reject $H_0$ if $Z < z_{\alpha/2}$ or $Z > z_{1 - \alpha/2}$

---
# Confidence intervals

--

- We can also construct confidence intervals in a similar manner to before

--

- A confidence interval at an $\alpha$ level of confidence is:
$$\hat{I} = \left[ \hat{\beta}_1 - z_{1 - \alpha/2} \cdot \sqrt{S_{\hat{\beta}_1}^2}, \hat{\beta}_1 + z_{1 - \alpha/2} \cdot \sqrt{S_{\hat{\beta}_1}^2} \right]$$

--

- Finally, hypothesis tests and confidence intervals for $\beta_0$ are conducted in an analogous manner

---
output:
  xaringan::moon_reader:
    seal: false
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
editor_options:
  chunk_output_type: console
---
class: center, inverse, middle

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra, echo = FALSE}
xaringanExtra::use_progress_bar(color = "#808080", location = "top")
```

```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.tiny123 {
  font-size: 0.40em;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}

.orange {
  color: orange
}

.green {
  color: green
}
```



# Statistics
## Estimating the mean using a simple random sample
### (Chapter 10)

### Seetha Menon,<br>Department of Economics<br>University of Southern Denmark

### Email: [smr@sam.sdu.dk](mailto:smr@sam.sdu.dk)

### Updated `r Sys.Date()`



---
class: inverse, middle, center
# Simple random sampling

---
# Descriptive measures in a sample

--

- Often, we want to derive information from a sample about the unknown distribution in the population

--

- There are various ways to characterize the population distribution, but it is common to use a descriptive measure

--

- The question is: how can we use the information in the sample to infer something about the value of descriptive measure in the population?

---
# Simple random sample

--

> A **simple random sample** is a sample $(X_1, X_2, \ldots, X_n)$ such that:
>
> 1. $X_1$, $X_2$, $\ldots$, $X_n$ are statistically independent
> 2. $X_1$, $X_2$, $\ldots$, $X_n$ have the same marginal distribution $f_X(x)$

--

- The marginal distribution $f_X(x)$ is the distribution of interest (in the population)

--

- Note that the second condition assures that the simple random sample is a representative sample for $X$

---
# Distribution of a simple random sample

--

- It is easy to determine the distribution of the random sample by using the two conditions in the definition of the sample:
$$
f(x_1, \ldots, x_n) = f_X(x_1) \cdot \ldots \cdot f_X(x_n)
$$

--

- So, if we knew the distribution of $X$ in the population, we would be able to determine the distribution of the simple random sample

---
# Distribution of a simple random sample

--

- For example, suppose $X$ follows a Bernoulli distribution with $p = 0.7$, and that $n = 3$

--

- Then we can calculate the probability of any realized sample:

--

$$
P(X_1 = 0, X_2 = 0, X_3 = 0) = P(X_1 = 0) \cdot P(X_2 = 0) \cdot P(X_3 = 0) = 0.3^3 = 0.027
$$

--

$$
P(X_1 = 0, X_2 = 0, X_3 = 1) = P(X_1 = 0) \cdot P(X_2 = 0) \cdot P(X_3 = 1) = 0.3^2 \cdot 0.7 = 0.063
$$

--

$$
\vdots
$$

--

$$
P(X_1 = 1, X_2 = 1, X_3 = 1) = P(X_1 = 1) \cdot P(X_2 = 1) \cdot P(X_3 = 1) = 0.7^3 = 0.343
$$

--

- But: we do not know the distribution of $X$ in the population...

---
class: inverse, middle, center
# Constructing an estimator of the mean value

---
# Estimating the mean value

--

- Recall that each element of the sample is a random variable following the same distribution

--

- In other words, each element of the sample (random variable) has the same mean value

--

- We could then use each realized value as an estimate (approximation) of the mean

--

- This gives us $n$ different estimates of one value $\Rightarrow$ we could use this information better and combine it into one single value

---
# The analogy principle

--

- The **analogy principle** of estimation says that we should use the formulas we would use if we knew the distribution in the population, but replace all the unknown population quantities with the corresponding sample quantities

--

- If we knew the distribution of the random variable $X$, that can take the values $x_1, x_2, \ldots, x_n$ with probabilities $f(x_1)$, $f(x_2)$, $\ldots$, $f(x_N)$, then its mean value is:
$$
\mu = \sum_{i=1}^N x_i f(x_i) = x_1 f(x_1) + x_2 f(x_2) + \ldots + x_N f(x_N)
$$

---
# The sample average

--

- In practice, we have a sample of $n$ values $x_1$, $x_2$, $\ldots$, $x_n$, each one occurring with probability $1/n$

--

- The analogy principle says that we should use these values in the formula above:
$$
\bar{x} = \sum_{i=1}^n x_i \cdot \frac{1}{n} = \frac{1}{n} \cdot \sum_{i=1}^n x_i
$$

--

- Since we can calculate this for any realized sample, we define the **sample average** as:
$$
\bar{X} = \sum_{i=1}^n X_i \cdot \frac{1}{n} = \frac{1}{n} \cdot \sum_{i=1}^n X_i
$$

---
# Estimate vs estimator

--

- Note that $\bar{X}$ is a *random variable*, while $\bar{x}$ is a value (and same for $\mu$)

--

- Actually, $\bar{x}$ is a realized value of $\bar{X}$

--

- $\bar{X}$ is an **estimator**, i.e., a random variable that is a function of the sample (random variables) and that aims to replicate a particular parameter in the population

--

- In contrast, $\bar{x}$ is an **estimate** (or **estimated value**), i.e., a realized value of the estimator, based on a particular realized sample

--

- The estimate is our guess of the true value of the parameter

---
class: inverse, middle, center
# Properties of estimators

---
# Properties of estimators

--

- We can construct an infinity of estimators, but we would want to have a *good* estimator

--

- We need to define what makes an estimator *good*, given that we do not know the true value of the parameter

--

- We are generally interested in three properties of estimators:

--

  - unbiasedness

--

  - efficiency

--

  - consistency

---
# Unbiasedness

--

- Recall that an estimator aims to approximate a particular parameter in the population

--

- Suppose we could sample many times and calculate the realized value of the estimator (the estimated value) in each sample

--

- We would want these values to be "around" the true value of the parameter

--

- In other words, we would want our estimator to be, "on average," equal to the true value of the parameter

--

- We say that an estimator is **unbiased** if its expected value is equal to the parameter that it aims to approximate

---
# Unbiasedness of sample average

--

- We can easily check if the sample average is unbiased:

--

$$
E(\bar{X}) = E \left[ \frac{1}{n} \sum_{i=1}^n X_i \right] = \frac{1}{n} \cdot E \left[ \sum_{i=1}^n X_i \right]
$$

--

$$
= \frac{1}{n} \cdot \sum_{i=1}^n E(X_i) = \frac{1}{n} \cdot n \cdot \mu
$$

--

$$
= \mu
$$

--

- Hence, the sample average is an unbiased estimator of the expected value of $X$

---
# Efficiency

--

- In practice, we cannot resample many times (we usually only have one realization of the sample)

--

- Since the estimator is a random variable, its realized value can be very far from the parameter of interest even if it is unbiased (i.e., its average value is equal to the parameter of interest)

--

- Therefore, we would like our estimator to take values "very close" to the true value of the parameter with high probability

--

- In other words, we would want the variance of the estimator to be as small as possible (the estimator would then be *precise*)

--

- An estimator is more **efficient** than another if its variance is smaller

---
# Variance of sample average

--

- We can also calculate relatively easily the variance of the sample average, because the sample elements are all independent of each other:

--

$$
Var(\bar{X}) = Var \left[ \frac{1}{n} \sum_{i=1}^n X_i \right] = \frac{1}{n^2} \cdot Var \left[ \sum_{i=1}^n X_i \right]
$$

--

$$
= \frac{1}{n^2} \cdot \sum_{i=1}^n Var(X_i) = \frac{1}{n^2} \cdot n \cdot \sigma^2
$$

--

$$
= \frac{\sigma^2}{n}
$$

--

- Note that the variance of the sample average depends on the sample size: the larger the sample, the smaller the variance (i.e., the estimator is more precise)

---
# Consistency

--

- Several estimators have the property that their variance is inversely proportional to sample size

--

- As sample size grows toward infinity, the variance of the estimator goes toward zero

--

- This means that in an infinite sample, the distribution of the estimator collapses on a single value

--

- If that single value is the true value of the parameter of interest (i.e., if the estimator is unbiased), then the estimator is **consistent**

--

- This implies that, if the sample is "sufficiently large," a consistent estimator will have realized values "very close" to the true value of the parameter

--

- In this case, even one realized sample may be enough to have a "good approximation" of the true value of the parameter

---
# Consistency of sample average

--

- Recall that the sample average is unbiased:
$$
E(\bar{X}) = \mu
$$

--

- Recall also that the variance of the sample average goes to zero as the sample size grows to infinity:
$$
\lim_{n \rightarrow \infty} Var(\bar{X}) = \lim_{n \rightarrow \infty} \frac{\sigma^2}{n} = 0
$$

--

- Therefore, the sample average is a consistent estimator of $\mu$

---
class: inverse, middle, center
# The distribution of the estimator $\bar{X}$

---
# The distribution of $\bar{X}$

--

- We calculated the expected value and variance of $\bar{X}$

--

- However, in certain situations we may want to calculate the probability that the observed value of this estimator lies in a certain interval

--

- For this we would need to know the entire distribution of this estimator, not just the values of these two moments

--

- If the shape of the distribution of $X$ in the population is known (i.e., the type of distribution, but not necessarily its parameters), then we may be able to determine the distribution of $\bar{X}$

---
# The distribution of $\bar{X}$ if sample is normal

--

- Suppose that the distribution of $X$ in the population is normal, with mean $\mu$ and variance $\sigma^2$

--

- Then each individual element of the sample also follows the same distribution: $X_i \sim \mathcal{N}(\mu, \sigma^2)$

--

- In this case, we will have: $\dfrac{X_i}{n} \sim \mathcal{N}\left(\dfrac{\mu}{n}, \dfrac{\sigma^2}{n^2} \right)$

--

- The sample average is:
$$
\bar{X} = \frac{X_1}{n} + \frac{X_2}{n} + \ldots + \frac{X_n}{n}
$$

--

- As a sum of normally-distributed random variables, $\bar{X}$ also follows normal distribution:
$$
\bar{X} \sim \mathcal{N}\left(\mu, \dfrac{\sigma^2}{n} \right)
$$

---
# The approximate distribution of $\bar{X}$

--

- In practice, we will not know the distribution of $X$ in the population

--

- Even if we do, we may not be able to determine the distribution of $\bar{X}$ so easily

--

- Luckily, we have the following very important result

--

> **Central limit theorem**
>
> Suppose we have a simple random sample of $n$ elements $X_1$, $X_2$, $\ldots$, $X_n$, where $E(X_i) = \mu$ and $Var(X_i) = \sigma^2$ for all $i$. If $n = \infty$, then the sample mean follows normal distribution: $\bar{X} \sim \mathcal{N}\left(\mu, \dfrac{\sigma^2}{n} \right)$

--

- This means that if the sample is "large enough," the distribution of $\bar{X}$ is *approximately normal*:
$$
\bar{X} \overset{a}{\sim} \mathcal{N}\left(\mu, \dfrac{\sigma^2}{n} \right)
$$

---
# Key Takeaways

--

- Estimating the mean in a simple random sample

--

- Estimate v. Estimator

--

- Properties of Estimators: Unbiased, Efficiency and Consistency

--

- Central limit theorem

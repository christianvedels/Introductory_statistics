---
output:
  xaringan::moon_reader:
    seal: false
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
editor_options:
  chunk_output_type: console
---
class: center, inverse, middle

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra, echo = FALSE}
xaringanExtra::use_progress_bar(color = "#808080", location = "top")
```

```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.tiny123 {
  font-size: 0.40em;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}

.orange {
  color: orange
}

.green {
  color: green
}
```



# Statistics
## Estimators of other descriptive measures
### (Chapter 12)

### Seetha Menon,<br>Department of Economics<br>University of Southern Denmark

### Email: [smr@sam.sdu.dk](mailto:smr@sam.sdu.dk)

### Updated `r Sys.Date()`



---
class: inverse, middle, center
# An estimator of the variance

---
# The analogy principle of estimation

--

- Recall that we constructed an estimator of the expected value (the sample average) using the analogy principle of estimation

--

- We can apply the same principle to construct an estimator of the variance

--

- The variance of a discrete random variable $X$ is:
$$
Var(X) = \sum_{i = 1}^N (x_i - \mu)^2 \cdot f_X(x_i)
$$

--

- To construct an estimator of the variance of $X$, $\sigma^2$, we need to replace all unknown quantities by corresponding sample quantities

---
# An estimator of the variance, $\mu$ known

--

- For simplicity, suppose we know the expected value of $X$ in the population, $\mu$

--

- The only unknown quantity is $f_X(x_i)$, which we can replace with $1/n$ (because each element in the sample has probability $1/n$)

--

- We then obtain the following estimator of $\sigma^2$:
$$
\tilde{S}^2 = \frac{1}{n} \cdot \sum_{i = 1}^n (X_i - \mu)^2
$$

--

- We can easily show that $\tilde{S}^2$ is an unbiased estimator of $\sigma^2$

---
# An estimator of the variance, $\mu$ unknown

--

- But, in general, we do not know the expected value of $X$ in the population, $\mu$, so we have two unknown quantities:

--

  - $f_X(x_i)$, which we can replace with $1/n$

--

  - $\mu$, which we can replace with the sample average $\bar{X}$

--

- We would then obtain the following estimator of $\sigma^2$:
$$
b^2 = \frac{1}{n} \cdot \sum_{i = 1}^n \left( X_i - \bar{X} \right)^2
$$

--

- However, $b^2$ is a biased estimator of $\sigma^2$
$$
E \left( b^2 \right) = \frac{n - 1}{n} \cdot \sigma^2
$$

--

- Therefore, we need to change the formula a bit to obtain an unbiased estimator of $\sigma^2$, called **sample variance**:
$$
S^2 = \frac{1}{n - 1} \cdot \sum_{i = 1}^n \left( X_i - \bar{X} \right)^2
$$

---
# An estimator of the standard deviation

--

- Recall that the standard deviation is the square root of the variance

--

- We could then use $S = \sqrt{S^2}$ as an estimator of the standard deviation of $X$

--

- However, recall that $E[g(X)]$ is in general not equal to $g[E(X)]$

--

- In this case, this means that:
$$
E(S) = E \left( \sqrt{S^2} \right) \not= \sqrt{E(S^2)} = \sqrt{\sigma^2} = \sigma
$$

--

- So $S$ is a *biased* estimator of $\sigma$ (but it is consistent)

---
# Properties of the sample variance

--

- As mentioned earlier, the sample variance is an unbiased (and consistent) estimator of the variance of $X$

--

- It turns out that it is easier to find the distribution of $Y$ than of $S^2$, where $Y$ is defined as:
$$
Y = S^2 \cdot \frac{n-1}{\sigma^2}
$$

--

- In an infinite sample, $Y$ follows a $\chi^2$ (*chi-square*) distribution with $n - 1$ degrees of freedom

--

- In a sufficiently large sample, $Y$ approximately follows a $\chi^2(n-1)$, which allows us to calculate the distribution of $S^2$:
$$
P(S^2 \leq a) = P \left( Y \cdot \frac{\sigma^2}{n-1} \leq a \right) = P \left( Y \leq a \cdot \frac{n-1}{\sigma^2} \right)
$$

--

- Unlike the normal distribution, the $\chi^2$ distribution is not symmetric: such a random variable can only take positive values

---
class: inverse, middle, center
# Estimators of higher-order moments

---
# Estimators of higher-order moments

--

- Recall that the $k$-th central moment of $X$ is defined as:
$$
m_k^* = E \left( [X - E(X)]^k \right)
$$

--

- If $X$ is a discrete variable, then we can calculate this moment as follows:
$$
m_k^* = \sum_{i=1}^N \left[ (x_i - \mu)^k \cdot f_X(x_i) \right]
$$

--

- The analogy principle gives us an estimator of this moment:
$$
\hat{m}_k^* = \frac{1}{n} \cdot \sum_{i=1}^n \left( X_i - \bar{X} \right)^k
$$

--

- Similarly, the estimator of the $k$-th moment, $E(X^k)$, is:
$$
\hat{m}_k = \frac{1}{n} \cdot \sum_{i=1}^n X_i^k
$$

---
# Skewness

--

- The moments depend on the unit of measurement of $X$ (e.g., if we change income from DKK to USD, the value of all of the moments changes)

--

- A scale-independent measure of **skewness** (asymmetry) of the distribution is the standardized third central moment:
$$
\gamma_3 = \frac{m_3^*}{\sigma^3}
$$

--

- Using the analogy principle, we can construct an estimator of this measure:
$$
\hat{\gamma}_3 = \frac{\hat{m}_3^*}{S^3}
$$

---
# Kurtosis

--

- The standardized fourth central moment is a measure of **kurtosis** (the "peakedness" of the probability distribution):
$$
\gamma_4 = \frac{m_4^*}{\sigma^4}
$$

--

- Its estimator is:
$$
\hat{\gamma}_4 = \frac{\hat{m}_4^*}{S^4}
$$

--

- Skewness and kurtosis are generally used to compare a distribution to the normal distribution

--

- For the normal distribution, $\gamma_3 = 0$ and $\gamma_4 = 3$

---
class: inverse, middle, center
# Estimators of the covariance and the correlation coefficient

---
# An estimator of the covariance

--

- Recall that the covariance between two random variables $X$ and $Y$ is defined as:
$$
Cov(X, Y) = E \left[ (X - \mu_X) \cdot (Y - \mu_Y) \right]
$$

--

- If both $X$ and $Y$ are discrete, then this formula becomes:
$$
Cov(X, Y) = \sum_{i=1}^N \sum_{j=1}^M \left[ (x_i - \mu_X) \cdot (y_j - \mu_Y) \cdot f(x_i, y_j) \right]
$$

--

- We can again use the analogy principle to construct an estimator of the covariance, called **sample covariance**:
$$
\widehat{Cov}(X, Y) = \frac{1}{n} \cdot \sum_{i=1}^n \left[ \left( X_i - \bar{X} \right) \cdot \left( Y_i - \bar{Y} \right) \right]
$$

---
# An estimator of the correlation coefficient

--

- Recall that the correlation coefficient between two random variables $X$ and $Y$ is defined as:
$$
\rho(X, Y) = \rho = \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}}
$$

--

- We can again use the analogy principle to construct an estimator called the **sample correlation coefficient**:
$$
\hat{\rho}(X, Y) = \frac{\widehat{Cov}(X, Y)}{\sqrt{S_X^2 \cdot S_Y^2}}
$$

--

- The sample correlation coefficient follows an approximate normal distribution:
$$
\hat{\rho}(X, Y) \overset{a}{\sim} \mathcal{N}\left(\rho, \dfrac{\left(1 - \rho^2 \right)^2}{n - 2} \right)
$$

---
class: inverse, middle, center
# Estimating a distribution

---
# Empirical distribution function

--

- Sometimes, estimating the moments of a distribution is not enough and we would like to have an idea of the shape of the distribution

--

- Based on a sample, we can derive an estimate of the cumulative distribution function $F_X(x)$

--

- Recall that the definition of the cumulative distribution function is:
$$
F_X(x) = P(X \leq x)
$$

--

- Using the analogy principle, this becomes: the probability that $X \leq x$ is the fraction of the sample with values lower than $x$

--

- Therefore, we can construct the **empirical distribution function**:
$$
\hat{F}_X(x) = \frac{\text{number of sample elements with } x_i \leq x}{n}
$$

---
# Properties of the empirical distribution function

--

- Suppose we have a simple random sample

--

- In this case, the empirical distribution function $\hat{F}_X(x)$ is an unbiased and consistent estimator of the cumulative distribution function:
$$
E \left[ \hat{F}_X(x) \right] = F_X(x) \quad \text{for all } x
$$

--

- We can also show that $\hat{F}_X(x)$ has an approximate normal distribution for all $x$:
$$
\hat{F}_X(x) \overset{a}{\sim} \mathcal{N} \left( F_X(x), \dfrac{F_X(x) \left[1 - F_X(x) \right]}{n} \right)
$$

---
class: inverse, middle, center
# Estimators of quantiles

---
# Estimated quantiles

--

- Quantiles are the second type of moments used to describe a distribution

--

- Recall that quantiles are defined based on the cumulative distribution function: the $p$-th quantile is the value $q_p$ at which the cumulative distribution function jumps from below $p$ to above $p$

--

- We can construct estimators of quantiles based on the empirical distribution function: the estimate of the $p$-th quantile is the value at which the empirical distribution function jumps from below $p$ to above $p$

--

- For example, the estimate of the median is the value at which the empirical distribution function jumps from below 0.5 to above 0.5

--

- If this happens over an interval of values, then the convention is to use the midpoint of the interval as the sample median

---
# Order statistics

--

- Another way of characterizing quantiles is to use **order statistics**

--

- The $k$-th order statistic is written as $X_{(k)}$ defined as the $k$-th smallest observation in the sample

--

- We can then define estimators for the $p$-th quantile using order statistics:
$$
\hat{q}_p =
\begin{cases}
X_{(n \cdot p + 1)}, & \text{if } [n \cdot p] \not= n \cdot p \\
\dfrac{X_{(n \cdot p)} + X_{(n \cdot p + 1)}}{2}, & \text{if } [n \cdot p] = n \cdot p
\end{cases}
$$
where $[n \cdot p]$ is the integer part of $n \cdot p$

--

- In the case of the sample median, this becomes:
$$
\hat{q}_{0.5} =
\begin{cases}
X_{\left( \frac{n + 1}{2} \right)}, & \text{if $n$ is odd} \\
\dfrac{X_{\left( \frac{n}{2} \right)} + X_{\left( \frac{n + 1}{2} \right)}}{2}, & \text{if $n$ is even}
\end{cases}
$$

---
class: inverse, middle, center
# Key Takeaways

---
# Key Takeaways: Variance

--

**Estimator of the variance**

--

**Population:**
$$Var(X) = \sum_{i = 1}^N (x_i - \mu)^2 \cdot f_X(x_i)$$

--

**Sample:**
$$S^2 = \frac{1}{n - 1} \cdot \sum_{i = 1}^n \left( X_i - \bar{X} \right)^2$$

--

The chi-square distribution allows us to approximate the distribution of $S^2$

---
# Key Takeaways: Higher-order moments

--

**Estimators of higher-order moments**

--

**Population:**
$$m_k^* = \sum_{i=1}^N \left[ (x_i - \mu)^k \cdot f_X(x_i) \right]$$

--

**Sample:**
$$\hat{m}_k^* = \frac{1}{n} \cdot \sum_{i=1}^n \left( X_i - \bar{X} \right)^k$$

---
# Key Takeaways: Skewness and Kurtosis

--

**Skewness** - A scale-independent measure of asymmetry (standardized third central moment)

--

**Population:** $\gamma_3 = \frac{m_3^*}{\sigma^3}$

**Sample:** $\hat{\gamma}_3 = \frac{\hat{m}_3^*}{S^3}$

--

**Kurtosis** - A scale-independent measure of peakedness (standardized fourth central moment)

--

**Population:** $\gamma_4 = \frac{m_4^*}{\sigma^4}$

**Sample:** $\hat{\gamma}_4 = \frac{\hat{m}_4^*}{S^4}$

---
# Key Takeaways: Covariance and Correlation

--

**Estimators of covariance and correlation coefficients**

--

**Population:**
$$Cov(X, Y) = \sum_{i=1}^N \sum_{j=1}^M \left[ (x_i - \mu_X) \cdot (y_j - \mu_Y) \cdot f(x_i, y_j) \right]$$

**Sample:**
$$\widehat{Cov}(X, Y) = \frac{1}{n} \cdot \sum_{i=1}^n \left[ \left( X_i - \bar{X} \right) \cdot \left( Y_i - \bar{Y} \right) \right]$$

--

**Population:** $\rho(X, Y) = \frac{Cov(X, Y)}{\sqrt{Var(X) \cdot Var(Y)}}$

**Sample:** $\hat{\rho}(X, Y) = \frac{\widehat{Cov}(X, Y)}{\sqrt{S_X^2 \cdot S_Y^2}}$

---
# Key Takeaways: Quantiles

--

**Estimators of quantiles**

--

**Population:** $q_p$

**Sample:** $\hat{q}_p$

--

Calculated exactly as done for the population, using the empirical distribution function or order statistics.

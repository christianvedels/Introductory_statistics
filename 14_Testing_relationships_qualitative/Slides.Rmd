---
output:
  xaringan::moon_reader:
    seal: false
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
editor_options:
  chunk_output_type: console
---
class: center, inverse, middle

```{r xaringan-panelset, echo=FALSE}
xaringanExtra::use_panelset()
```

```{r xaringan-tile-view, echo=FALSE}
xaringanExtra::use_tile_view()
```

```{r xaringanExtra, echo = FALSE}
xaringanExtra::use_progress_bar(color = "#808080", location = "top")
```

```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.tiny123 {
  font-size: 0.40em;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}

.orange {
  color: orange
}

.green {
  color: green
}
```



# Statistics
## Testing relationships using qualitative data
### (Chapter 16)

### Seetha Menon,<br>Department of Economics<br>University of Southern Denmark

### Email: [smr@sam.sdu.dk](mailto:smr@sam.sdu.dk)

### Updated `r Sys.Date()`



---
class: inverse, middle, center
# The $\chi^2$-test

---
# Motivation

--

- We will now turn to testing different properties of discrete random variables

--

- Discrete random variables arise in practice when we deal with qualitative data

--

- For example:
  - highest level of education attained
  - gender
  - type of car purchased (gas, diesel, hybrid, electric)

---
# The $\chi^2$ test

--

- We can test whether the observed (empirical) distribution of a discrete random variable is similar to a given distribution

--

- Let $X$ be the variable of interest, and suppose that it can only take $K$ values $x_1$, $x_2$, $\ldots$, $x_K$

--

- The $K$ values are usually called **categories**

--

- The probability of each category $k$ is $p_k$:
$$P(X = x_k) = p_k$$

--

- The hypotheses of the $\chi^2$ test are:
$$\begin{align*}
  H_0 &:  p_1 = \pi_1, \;  p_2 = \pi_2, \ldots, p_K = \pi_K, \\
  H_1 &:  \text{at least one } p_k \not= \pi_k
\end{align*}$$
where $\pi_1$, $\pi_2$, $\ldots$, $\pi_K$ are the probabilities from the given distribution

--

- For the moment being, we will assume that this distribution is given (we will see later on how we can choose it)

---
# Hypothesis measure

--

- We can construct a hypothesis measure that is equal to zero under the null hypothesis and different from zero otherwise:
$$h(p_1, p_2, \ldots, p_K) = \sum_{k=1}^K (p_k - \pi_k)^2$$

--

- As before, the $p_k$'s are unknown, so we need to replace them with observed quantities

--

- Suppose we have a simple random sample $(X_1, X_2, \ldots, X_n)$ from this distribution

--

- Let $Z_k$ be the number of elements in the sample that have the value $x_k$

--

- Then an estimator for $p_k$ is simply $Z_k / n$

---
# Test statistic

--

- In order for the test statistic to have a "nice" distribution, we need to change the formula a little bit

--

- In particular, it can be shown that the following test statistic follows a $\chi^2$ distribution with $(K - 1)$ degrees of freedom under the null hypothesis:
$$\chi^2 = \sum_{k=1}^K \frac{(Z_k - n \cdot \pi_k)^2}{n \cdot \pi_k}$$

--

- The number of degrees of freedom is $(K - 1)$ because we can only freely assign values to $K - 1$ probabilities (the $K$-th one is 1 minus the others)

--

- If we conduct a hypothesis test at a $(1 - \alpha)$ confidence level, then we reject the null hypothesis if we observe too large values of $\chi^2$:
  - do not reject $H_0$ if $\chi^2 \leq \chi^2_{1 - \alpha}(K-1)$
  - reject $H_0$ if $\chi^2 > \chi^2_{1 - \alpha}(K-1)$

---
class: inverse, middle, center
# Test of a distribution

---
# Known distribution

--

- We can use the $\chi^2$ test to check if the sample comes from a given distribution

--

- For example, suppose that we roll a dice and we record the result of every roll

--

- We would want to compare the resulting distribution to the theoretical distribution from an "unbiased" dice: $\pi_1 = \pi_2 = \cdots = \pi_6 = \frac{1}{6}$

--

- We can conduct a similar test if we want to check if a coin is "loaded" by comparing the observed distribution to the theoretical one: $\pi_1 = \pi_2 = 0.5$

---
# Unknown distribution

--

- Sometimes, we may guess the shape of the distribution, but not its particular parameters

--

- For example, we may know that the sample comes from a binomial distribution and one of the parameters ( $n$ ), but not the other ( $p$ )

--

- In this case, we can use the sample to obtain an estimate of this unknown parameter, and use this parameter to calculate the probabilities from the theoretical distribution

--

- The test statistic would then follow a $\chi^2$ distribution with $(K - 2)$ degrees of freedom (we "lose" one more degree of freedom because we need to estimate $p$ )

---
class: inverse, middle, center
# Testing the relationship between two discrete random variables

---
# Independence

--

- Recall that if two random variables are independent, then knowing the value of one of them does not allow us to infer anything about the value of the other

--

- We can use a $\chi^2$ test to check if two discrete random variables are indeed independent

---
# Hypotheses

--

- Recall that two random variables $X$ and $Y$ are independent if and only if the joint probability function is equal to the product of the marginal probability functions:
$$f(x, y) = f_X(x) \cdot f_Y(y)$$

--

- Therefore, the hypotheses tested are:
$$\begin{align*}
  H_0 &: \text{Independence, that is, } f(x, y) = f_X(x) \cdot f_Y(y) \text{ for all } x, y, \\
  H_1 &: \text{Dependence, that is, } f(x, y) \not= f_X(x) \cdot f_Y(y) \text{ for some } x, y
\end{align*}$$

--

- We can interpret $f(x, y)$ as the unknown probabilities, and $f_X(x) \cdot f_Y(y)$ as the chosen distribution to be tested

---
# Setup

--

- Suppose $X$ can take $J$ possible values $x_1$, $x_2$, $\ldots$, $x_J$, and $Y$ can take $L$ different values $y_1$, $y_2$, $\ldots$, $y_L$

--

- We then have $K = J \cdot L$ categories

--

- We do not know $f_X(x)$ nor $f_Y(y)$, but we can again use the sample to obtain estimates

--

- Let us assume that we have a simple random sample of $n$ observations $((X_1, Y_1)$, $(X_2, Y_2)$, $\ldots$, $(X_n, Y_n))$

---
# Probabilities

--

- We can construct estimates of the marginal probability functions in a similar way to before:
$$\begin{align*}
  \hat{f}_X(x_j) & = \frac{\text{number of sample elements with } X = x_j}{n} \\
  \hat{f}_Y(y_l) & = \frac{\text{number of sample elements with } Y = y_l}{n}
\end{align*}$$

--

- The joint probability function is just the observed (empirical) distribution:
$$f(x_j, y_l) = \frac{\text{number of sample elements with } X = x_j \text{ and } Y = y_l}{n}$$

---
# Test statistic

--

- We can then construct the test statistic in a similar way to before:
$$\chi^2 = \sum_{j=1}^J \sum_{l=1}^L \frac{\left[Z_{jl} - n \cdot \hat{f}_X(x_j) \cdot \hat{f}_Y(y_l)\right]^2}{n \cdot \hat{f}_X(x_j) \cdot \hat{f}_Y(y_l)}$$

--

- This test statistic follows a $\chi^2$ distribution

--

- In order to calculate it, we "lost" a number of degrees of freedom:
  - $J - 1$, to calculate the marginal probability function $\hat{f}_X(x)$
  - $L - 1$, to calculate the marginal probability function $\hat{f}_Y(y)$

--

- Therefore, the test statistic follows a $\chi^2$ distribution with $(J - 1) \cdot (L-1)$ degrees of freedom

---
class: inverse, middle, center
# Test for homogeneity

---
# Homogeneity

--

- We can also test for homogeneity, that is, if the distribution is the same across groups

--

- For example, we may want to test if the age distribution of men and of women are the same

--

- Let $X$ be the characteristic of interest (age), and $Y$ the characteristic that defines the groups ( $Y = 1$ for men and $Y = 2$ for women)

--

- A test of homogeneity is a comparison of the conditional distributions $f_{X|Y}(x | Y = 1)$ and $f_{X|Y}(x | Y = 2)$

--

- But recall that if $f_{X|Y}(x | Y = 1) = f_{X|Y}(x | Y = 2)$ for every $x$, this means that $X$ and $Y$ are independent

--

- So a test of homogeneity of $X$ between groups defined by $Y$ is simply a test of independence between $X$ and $Y$
